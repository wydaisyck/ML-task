{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906f1ac5-1136-4f15-a467-b95d79d9ddc6",
   "metadata": {
    "id": "906f1ac5-1136-4f15-a467-b95d79d9ddc6"
   },
   "source": [
    "# HW5\n",
    "## Yidan Wang 2973331278"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd48c3-9419-4881-a7d3-5e16831062f5",
   "metadata": {
    "id": "9bbd48c3-9419-4881-a7d3-5e16831062f5"
   },
   "source": [
    "### 1. Multi-class and Multi-Label Classiﬁcation Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb112845-0e2d-4056-aad6-ecae77f946af",
   "metadata": {
    "id": "eb112845-0e2d-4056-aad6-ecae77f946af"
   },
   "source": [
    "#### (a) process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d40e1414-53a3-4bb6-930d-97ab6f0ecac8",
   "metadata": {
    "executionInfo": {
     "elapsed": 1676,
     "status": "ok",
     "timestamp": 1636586039612,
     "user": {
      "displayName": "Yidan Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08126706267337521572"
     },
     "user_tz": 480
    },
    "id": "d40e1414-53a3-4bb6-930d-97ab6f0ecac8"
   },
   "outputs": [],
   "source": [
    "# load the data and packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_data = pd.read_csv('../data/Anuran Calls (MFCCs)/Frogs_MFCCs.csv')\n",
    "\n",
    "\n",
    "x_data = raw_data.iloc[:,:-4]\n",
    "y = raw_data.iloc[:,-4:-1]\n",
    "\n",
    "X_train,X_test, Y_train_all, Y_test_all = train_test_split(x_data, y, test_size=0.3, random_state=434, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37522d60-77b5-45ae-9da4-217ccc558441",
   "metadata": {
    "id": "37522d60-77b5-45ae-9da4-217ccc558441"
   },
   "source": [
    "#### (b) Each instance has three labels: Families, Genus, and Species. Each of the labels has multiple classes. We wish to solve a multi-class and multi-label problem. One of the most important approaches to multi-label classiﬁcation is to train a classiﬁer for each label (binary relevance). We ﬁrst try this approach:\n",
    "\n",
    "#### i. Research exact match and hamming score/ loss methods for evaluating multilabel classiﬁcation and use them in evaluating the classiﬁers in this problem.\n",
    "\n",
    "#### we can use svm.score(X, y, sample_weight=None) to get the exact match. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.\n",
    "#### we can use sklearn.metrics.hamming_loss(y_true, y_pred, *, sample_weight=None) to get the hamming loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d7391-fd33-4081-990a-9d8096f1b95b",
   "metadata": {
    "id": "5b7d7391-fd33-4081-990a-9d8096f1b95b"
   },
   "source": [
    "#### ii. Train a SVM for each of the labels, using Gaussian kernels and one versus all classiﬁers. Determine the weight of the SVM penalty and the width of the Gaussian Kernel using 10 fold cross validation. 1 You are welcome to try to solve the problem with both standardized 2 and raw attributes and report the results.\n",
    "\n",
    "#### Answer:\n",
    "#### from the results shown below, we can see the raw attributes are better than the standardized attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e09c425-687b-4278-882d-69bcbeb0dad7",
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1636586075307,
     "user": {
      "displayName": "Yidan Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08126706267337521572"
     },
     "user_tz": 480
    },
    "id": "8e09c425-687b-4278-882d-69bcbeb0dad7"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# find the C range roughly\n",
    "def find_c(k, X_train, y_train):\n",
    "    C_range_scale = np.logspace(-3, 5, 9)\n",
    "    C_range = []\n",
    "    for c in C_range_scale:\n",
    "        clf = SVC(decision_function_shape='ovr', C=c, kernel=k)\n",
    "        clf.fit(X_train, y_train)\n",
    "        s_score = clf.score(X_train, y_train)\n",
    "        if s_score == 1:\n",
    "            C_range.append(c)\n",
    "            break\n",
    "        elif s_score > 0.7:\n",
    "            C_range.append(c)\n",
    "    return np.array(C_range)\n",
    "\n",
    "def CV_processing(k, X_train, y_train, class_name):\n",
    "    C_range = find_c(k=k, X_train = X_train, y_train=y_train)\n",
    "    gamma_range = np.linspace(0.1, 3.5, 18)\n",
    "    param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "    grid = GridSearchCV(SVC(decision_function_shape='ovr', kernel=k), param_grid=param_grid, cv=10, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\n",
    "    \"The best parameters of %s are %s with a CV score of %0.2f\"\n",
    "    % (class_name, grid.best_params_, grid.best_score_))\n",
    "    return grid.best_params_['C'], grid.best_params_['gamma']\n",
    "\n",
    "def fit_predict_result(k, class_name, X_train = X_train, X_test = X_test):\n",
    "    y_train = Y_train_all[class_name]\n",
    "    y_test = Y_test_all[class_name]\n",
    "    c, g = CV_processing(k, X_train, y_train, class_name)\n",
    "    model = SVC(kernel=k, C=c, gamma=g, decision_function_shape='ovr')\n",
    "    model.fit(X_train, y_train)\n",
    "    emr = model.score(X_test, y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    hl = hamming_loss(y_test, y_pred)\n",
    "    return emr, hl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e92381-1b40-4a0e-b9c9-d0ec0b4e6907",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3101283,
     "status": "ok",
     "timestamp": 1636589176772,
     "user": {
      "displayName": "Yidan Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08126706267337521572"
     },
     "user_tz": 480
    },
    "id": "85e92381-1b40-4a0e-b9c9-d0ec0b4e6907",
    "outputId": "7644ab26-72ad-464e-e7e4-640df60bad42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Kernel:\n",
      "Raw attributes:\n",
      "The best parameters of Family are {'C': 10.0, 'gamma': 2.5} with a CV score of 0.99\n",
      "For class: Family, the test score(exact match rate) is 0.9967577582213988, while the hamming loss is 0.003242241778601204.\n",
      "\n",
      "\n",
      "The best parameters of Genus are {'C': 10.0, 'gamma': 1.5} with a CV score of 0.99\n",
      "For class: Genus, the test score(exact match rate) is 0.9921259842519685, while the hamming loss is 0.007874015748031496.\n",
      "\n",
      "\n",
      "The best parameters of Species are {'C': 10.0, 'gamma': 1.5} with a CV score of 0.99\n",
      "For class: Species, the test score(exact match rate) is 0.9930523390458545, while the hamming loss is 0.006947660954145438.\n",
      "\n",
      "\n",
      "Standardized attributes:\n",
      "The best parameters of Family are {'C': 10.0, 'gamma': 0.1} with a CV score of 0.99\n",
      "For class: Family, the test score(exact match rate) is 0.9930523390458545, while the hamming loss is 0.006947660954145438.\n",
      "\n",
      "\n",
      "The best parameters of Genus are {'C': 10.0, 'gamma': 0.1} with a CV score of 0.99\n",
      "For class: Genus, the test score(exact match rate) is 0.9916628068550255, while the hamming loss is 0.008337193144974525.\n",
      "\n",
      "\n",
      "The best parameters of Species are {'C': 10.0, 'gamma': 0.1} with a CV score of 0.98\n",
      "For class: Species, the test score(exact match rate) is 0.9884205650764243, while the hamming loss is 0.01157943492357573.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for raw X\n",
    "\n",
    "print(\"Gaussian Kernel:\")\n",
    "print(\"Raw attributes:\")\n",
    "for class_name in [\"Family\", \"Genus\", \"Species\"]:\n",
    "    k = \"rbf\" # default\n",
    "    emr_1, hl_1 = fit_predict_result(k = k, class_name = class_name)   \n",
    "    print(\"For class: %s, the test score(exact match rate) is %s, while the hamming loss is %s.\"\n",
    "         % (class_name, emr_1, hl_1))\n",
    "    print(\"\\n\")  \n",
    "\n",
    "    \n",
    "# for std X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "train_x_st = scaler.transform(X_train)\n",
    "test_x_st = scaler.transform(X_test)\n",
    "\n",
    "print(\"Standardized attributes:\")\n",
    "for class_name in [\"Family\", \"Genus\", \"Species\"]:\n",
    "    k = \"rbf\" # default\n",
    "    emr_2, hl_2 = fit_predict_result(k = k, X_train = train_x_st, X_test = test_x_st, class_name = class_name)   \n",
    "    print(\"For class: %s, the test score(exact match rate) is %s, while the hamming loss is %s.\"\n",
    "         % (class_name, emr_2, hl_2))\n",
    "    print(\"\\n\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f6210-ce55-4068-8dc9-a3db09787d77",
   "metadata": {
    "id": "384f6210-ce55-4068-8dc9-a3db09787d77"
   },
   "source": [
    "#### iii. Repeat 1(b)ii with L1 -penalized SVMs. Remember to standardize the attributes. Determine the weight of the SVM penalty using 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7126d51d-e488-4a56-b72d-0d800fac9ce6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42130,
     "status": "ok",
     "timestamp": 1636589516949,
     "user": {
      "displayName": "Yidan Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08126706267337521572"
     },
     "user_tz": 480
    },
    "id": "7126d51d-e488-4a56-b72d-0d800fac9ce6",
    "outputId": "1199d07e-822c-4b82-86fa-3f28050d0119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel:\n",
      "The best parameters of Family are {'C': 1000.0} with a CV score of 0.93\n",
      "For class: Family, the test score(exact match rate) is 0.9411764705882353, while the hamming loss is 0.058823529411764705.\n",
      "\n",
      "\n",
      "The best parameters of Genus are {'C': 1.0} with a CV score of 0.95\n",
      "For class: Genus, the test score(exact match rate) is 0.9541454377026402, while the hamming loss is 0.04585456229735989.\n",
      "\n",
      "\n",
      "The best parameters of Species are {'C': 10.0} with a CV score of 0.96\n",
      "For class: Species, the test score(exact match rate) is 0.9615562760537286, while the hamming loss is 0.03844372394627142.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "def CV_processing_2(X_train, y_train, class_name):\n",
    "    C_range = np.logspace(-3, 6, 10)\n",
    "    param_grid = dict(C=C_range)\n",
    "    grid = GridSearchCV(LinearSVC(penalty=\"l1\", dual=False, max_iter=10000), param_grid=param_grid, cv=10, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\n",
    "    \"The best parameters of %s are %s with a CV score of %0.2f\"\n",
    "    % (class_name, grid.best_params_, grid.best_score_))\n",
    "    return grid.best_params_['C']\n",
    "\n",
    "def fit_predict_result_2(class_name, X_train = X_train, X_test = X_test):\n",
    "    y_train = Y_train_all[class_name]\n",
    "    y_test = Y_test_all[class_name]\n",
    "    c = CV_processing_2(X_train, y_train, class_name)\n",
    "    model = LinearSVC(penalty=\"l1\", C=c, dual=False, max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "    emr = model.score(X_test, y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    hl = hamming_loss(y_test, y_pred)\n",
    "    return emr, hl\n",
    "\n",
    "print(\"Linear Kernel:\")\n",
    "\n",
    "for class_name in  [\"Family\", \"Genus\", \"Species\"]:\n",
    "    emr_3, hl_3 = fit_predict_result_2(X_train = train_x_st, X_test = test_x_st, class_name = class_name)   \n",
    "    print(\"For class: %s, the test score(exact match rate) is %s, while the hamming loss is %s.\"\n",
    "         % (class_name, emr_3, hl_3))\n",
    "    print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45740791-261b-4fc5-b90f-23b03c942a66",
   "metadata": {
    "id": "45740791-261b-4fc5-b90f-23b03c942a66"
   },
   "source": [
    "#### iv. Repeat 1(b)iii by using SMOTE or any other method you know to remedy class imbalance. Report your conclusions about the classiﬁers you trained.\n",
    "\n",
    "#### use linear kernal and smote in standardized attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aa1fad4-2988-4518-a1bd-0220347cb70d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 459475,
     "status": "ok",
     "timestamp": 1636592897117,
     "user": {
      "displayName": "Yidan Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08126706267337521572"
     },
     "user_tz": 480
    },
    "id": "6aa1fad4-2988-4518-a1bd-0220347cb70d",
    "outputId": "dcadff21-5915-426b-d94a-36a150976431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel with SMOTE:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters of Family are {'clf__C': 10.0} with a CV score of 0.92\n",
      "For class: Family, the test score(exact match rate) is 0.9615562760537286, while the hamming loss is 0.03844372394627142.\n",
      "\n",
      "\n",
      "The best parameters of Genus are {'clf__C': 10.0} with a CV score of 0.92\n",
      "For class: Genus, the test score(exact match rate) is 0.9615562760537286, while the hamming loss is 0.03844372394627142.\n",
      "\n",
      "\n",
      "The best parameters of Species are {'clf__C': 0.1} with a CV score of 0.95\n",
      "For class: Species, the test score(exact match rate) is 0.9615562760537286, while the hamming loss is 0.03844372394627142.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def CV_processing_3(X_train, y_train, class_name):\n",
    "    C_range = np.logspace(-3, 6, 10)\n",
    "    param_grid = {'clf__C' : C_range},\n",
    "    model = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=424, n_jobs=-1)),\n",
    "        ('clf', LinearSVC(penalty=\"l1\", dual=False, max_iter=10000))\n",
    "    ])\n",
    "    grid = GridSearchCV(model, param_grid=param_grid, cv=10, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\n",
    "    \"The best parameters of %s are %s with a CV score of %0.2f\"\n",
    "    % (class_name, grid.best_params_, grid.best_score_))\n",
    "    return grid.best_params_['clf__C']\n",
    "\n",
    "def fit_predict_result_3(class_name, X_train = X_train, X_test = X_test):\n",
    "    y_train = Y_train_all[class_name]\n",
    "    y_test = Y_test_all[class_name]\n",
    "    c = CV_processing_3(X_train, y_train, class_name)\n",
    "    model = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=424, n_jobs=-1)),\n",
    "        ('clf', LinearSVC(penalty=\"l1\", C=c, dual=False, max_iter=10000))\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    emr = model.score(X_test, y_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "    hl = hamming_loss(y_test, y_pred)\n",
    "    return emr, hl\n",
    "\n",
    "print(\"Linear Kernel with SMOTE:\")\n",
    "\n",
    "for class_name in  [\"Family\", \"Genus\", \"Species\"]:\n",
    "    emr_4, hl_4 = fit_predict_result_3(X_train = train_x_st, X_test = test_x_st, class_name = class_name)   \n",
    "    print(\"For class: %s, the test score(exact match rate) is %s, while the hamming loss is %s.\"\n",
    "         % (class_name, emr_3, hl_3))\n",
    "    print(\"\\n\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d3d3d-23cd-434e-880f-ca6f43ec7ab9",
   "metadata": {
    "id": "f83d3d3d-23cd-434e-880f-ca6f43ec7ab9"
   },
   "source": [
    "\n",
    "#### In my mind, there is no need to do the Chain method, because using binary relevance and Gaussian kernel, the test score can reach to 0.99."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "hw5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
