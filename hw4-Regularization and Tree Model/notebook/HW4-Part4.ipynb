{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4711d87-a221-4356-a0aa-20a7de0039f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PART4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b20c1a-55a6-485e-9410-72c057487675",
   "metadata": {},
   "source": [
    "### 3. ISLR 6.6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbf75b0-5d4d-4105-8b72-e92c803c683c",
   "metadata": {},
   "source": [
    "#### Suppose we estimate the regression coeﬃcients in a linear regression model by minimizing\n",
    "\n",
    "\n",
    "$\\sum_{i=1}^n\\Biggl(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\Biggr)\\text{ subject to }\\sum_{j=1}^p|\\beta_j|\\le s$  \n",
    "#### for a particular value of s. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer.\n",
    "\n",
    "(a) As we increase s from 0, the training RSS will:\n",
    "\n",
    "i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
    "\n",
    "ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "\n",
    "iii. Steadily increase.\n",
    "\n",
    "iv. Steadily decrease.\n",
    "\n",
    "v. Remain constant.\n",
    "\n",
    "(b) Repeat (a) for test RSS.\n",
    "\n",
    "(c) Repeat (a) for variance.\n",
    "\n",
    "(d) Repeat (a) for (squared) bias.\n",
    "\n",
    "(e) Repeat (a) for the irreducible error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aad10b-a4d7-4983-81aa-270a2bc37468",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "#### (a) The train RSS will iii.steadily decrease  \n",
    "\n",
    "when s=0 we have the intercept, which is the expected mean value. As s→∞, we are restricting the βj coefficients less and less (the coefficients will increase to their least squares estimates), and so the model is becoming more and more flexible. We will see a decrease in the bias and therefore a steady decrease in the training RSS as we approach a least squares regression.\n",
    "\n",
    "#### (b) the test RSS will ii. Decrease initially, and then eventually start increasing in a U shape.  \n",
    "\n",
    "The model is becoming more and more flexible which provokes at first a decrease in the test RSS and then starts to overfit the training data and lead to Test RSS increase.\n",
    "\n",
    "#### (c) the variance will iii. Steadily increase.  \n",
    "\n",
    "Variance steadily increase with the increase in model flexibility\n",
    "\n",
    "#### (d) the (squared) bias will iv. Steadily decrease.  \n",
    "\n",
    "Bias decreases with the increase in the model flexibilit\n",
    "\n",
    "#### (e) the irreducible error will v. Remain constant.\n",
    "\n",
    "Irreducible error is independent of model parameters and thus independent of s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98127b8-3d54-4afb-a49e-24c69903cc13",
   "metadata": {},
   "source": [
    "### 4. ISLR 6.6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a7b77-e3d7-4834-8ab5-bcd013e4f7b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give different coefficient values to correlated variables. We will now explore this property in a very simple setting.\n",
    "\n",
    "#### Suppose that n=2, p=2, x11=x12, x21=x22. Furthermore, suppose that y1+y2=0 and x11+x21=0 and x12+x22=0, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero : β̂ 0=0.\n",
    "\n",
    "(a) Write out the ridge regression optimization problem in this setting.\n",
    "\n",
    "(b) Argue that in this setting, the ridge coeﬃcient estimates satisfy $\\hat\\beta_1 = \\hat\\beta_2\n",
    "$ .\n",
    "\n",
    "(c) Write out the lasso optimization problem in this setting.\n",
    "\n",
    "(d) Argue that in this setting, the lasso coeﬃcients β 1 ˆ and β 2 ˆ are not unique—in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649e297-e3cc-461e-a7a7-0ad56022283e",
   "metadata": {},
   "source": [
    "![jupyter](./4-1.png)\n",
    "![jupyter](./4-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096ab14-3a4a-4121-8959-a20ae3efeaba",
   "metadata": {},
   "source": [
    "### 5. ISLR 8.4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb5ada-0d46-4d03-8b36-0a71ec2ea493",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of X, produce 10 estimates of P(Class is Red|X) :\n",
    "#### 0.1,0.15,0.2,0.2,0.55,0.6,0.6,0.65,0.7,0.75.\n",
    "#### There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability. In this example, what is the final classification under each of these two approaches ?\n",
    "\n",
    "ANSWER:\n",
    "#### With the majority vote approach, we classify X as Red (6 for Red vs 4 for Green). \n",
    "#### With the average probability approach, we classify X as Green as the average of the 10 probabilities is 4.5/10 = 0.45."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa942f6-c2b9-45f2-b313-cb30e5d86106",
   "metadata": {},
   "source": [
    "### 6. ISLR 9.7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382ec98-f2dc-47d5-9582-153bf21a0ec8",
   "metadata": {},
   "source": [
    "3. Here we explore the maximal margin classiﬁer on a toy data set.\n",
    "\n",
    "(a) We are given n = 7 observations in p = 2 dimensions. For each observation, there is an associated class label. Sketch the observations.\n",
    "\n",
    "(b) Sketch the optimal separating hyperplane, and provide the equation for this hyperplane (of the form (9.1)).\n",
    "\n",
    "(c) Describe the classiﬁcation rule for the maximal margin classiﬁer. It should be something along the lines of “Classify to Red if β 0 + β 1 X 1 + β 2 X 2 > 0, and classify to Blue otherwise.” Provide\n",
    "\n",
    "the values for β 0 , β 1 , and β 2 .\n",
    "\n",
    "(d) On your sketch, indicate the margin for the maximal margin hyperplane.\n",
    "\n",
    "(e) Indicate the support vectors for the maximal margin classiﬁer.\n",
    "\n",
    "(f) Argue that a slight movement of the seventh observation would not aﬀect the maximal margin hyperplane.\n",
    "\n",
    "(g) Sketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane.\n",
    "\n",
    "(h) Draw an additional observation on the plot so that the two classes are no longer separable by a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1246dc2a-b2d5-4624-a7dd-5da832485519",
   "metadata": {},
   "source": [
    "![jupyter](./6-1.png)\n",
    "![jupyter](./6-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda9d79-c5c1-4858-97f4-d110773b9e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
